# Hello, World!
Hello! ðŸ‘‹

I am Thiago Farias ([LinkedIn Profile](https://www.linkedin.com/in/fariasthiago/?locale=en_US)), and this is a practical exercise in Data Engineering, using Databricks. Here's a detailed guide on the project and its functionalities.

## Dataset
For this project, I chose the [Bike Store Relational Database](https://www.kaggle.com/datasets/dillonmyrick/bike-store-sample-database). The Entity-Relationship (ER) diagram provided can be seen below:

![Schema Bike Store](https://i.imgur.com/04YEup6.png)


## Bronze Layer

In this stage, I performed the ingestion and initial preparation of the Kaggle dataset. Below is an overview of the steps taken in this layer:

1. **Kaggle Authentication**: Authenticated to Kaggle using the provided credentials to access the dataset.
2. **Download and Storage**: Downloaded the dataset files from Kaggle and stored them in the DBFS (Databricks File System) in the bronze layer.
3. **Export to Delta Lake and Parquet**: Exported the DataFrames to Delta Lake and Parquet formats in the bronze layer for easier processing and analysis later on.

These steps are essential for preparing the data for the subsequent transformation and analysis stages in the silver layer.

## Silver Layer

In the silver layer, I performed the following data transformations to prepare them for deeper analysis:

1. **Conversion of 'NULL' Values**: Iterated over each DataFrame to replace 'NULL' values, making the representation of null values more consistent and readable.
2. **Duplicate Checking and Removal**: Checked and removed duplicates in all DataFrames to ensure data integrity.
3. **DataFrame-Specific Transformations**:
   - **ORDERS**: Converted the "shipped_date" field to Date type and applied formatting.
   - **STAFFS**: Converted the "manager_id" field from string to integer and created the "full_name" column by concatenating "first_name" and "last_name".
   - **CUSTOMERS**: Created the "full_name" column by concatenating "first_name" and "last_name", and converted the "zip_code" column to string.
   - **STORES**: Converted the "zip_code" column to string.
4. **Export to Delta Lake and Parquet**: Exported the DataFrames to Delta Lake and Parquet formats in the silver layer.

These transformations are crucial for cleaning and preparing the data for further analysis in the gold layer.

## Gold Layer

In the gold layer, I conducted advanced data analysis to extract valuable insights. Below are some of the analyses and visualizations performed:

1. **Revenue Distribution by Store**:
   - SQL query to calculate the total revenue per store.
   - Bar chart showing the revenue distribution by store.

2. **Top 10 Customers by Total Spending**:
   - SQL query to identify the top 10 customers who spent the most.
   - Horizontal bar chart displaying the top 10 customers with the highest total spending.

3. **Revenue Distribution by Year and Month**:
   - SQL queries to calculate the total revenue by year and by month.
   - Bar charts showing the revenue distribution over time, both by year and by month.

4. **Top 10 Products by Total Revenue**:
   - SQL query to identify the top 10 products that generated the most revenue.
   - Horizontal bar chart displaying the top 10 products with the highest total revenue.

5. **Total Revenue by Seller**:
   - SQL query to calculate the total revenue generated by each seller.
   - Horizontal bar chart showing the total revenue by seller.

6. **RFV (Recency, Frequency, Value) Analysis**:
   - SQL query to segment customers based on purchase frequency, recency of last purchase, and amount spent.
   - Count plots showing the distribution of customers in different segments.

7. **30-Day Moving Average for Orders**:
   - SQL query to calculate the 30-day moving average for the number of orders.
   - Line chart showing the 30-day moving average for each store over time.

8. **Average Units Sold per Month of the Year, by Product Category**:
   - SQL query to calculate the average units sold per month for each product category.
   - Bar chart showing the average units sold per month of the year for each product category.

These analyses and visualizations provide a deeper understanding of the data and can guide strategic decisions for the company.


## SQL Data Warehouse
I loaded the data into permanent tables in the BikeStore database to facilitate access and analysis through SQL queries and Power BI.

![DW](https://i.imgur.com/Hk6k6sA.png)

# Dashboards
- [Databricks](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1802048069101167/3500734898195404/2682982357215681/latest.html)
- [Power BI](https://app.powerbi.com/view?r=eyJrIjoiNTJlOGJkNmYtYTQ3OS00Mzc1LWJhYTEtM2Q0YTA1NmMzMWM5IiwidCI6ImFjYmJkZDFlLTE4YWYtNDIyMy04ZTdiLWMwZDk3MTllYTVmZiJ9&pageName=ReportSection)

![Dashboard](https://i.imgur.com/yjMC5Ax.png)

## Thank you for visiting!

Thank you for checking out my portfolio. If you need more information or want to discuss any project, feel free to reach out.
